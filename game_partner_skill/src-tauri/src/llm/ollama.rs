use crate::settings::ModelConfig;
use anyhow::{anyhow, Result};
use serde::{Deserialize, Serialize};

/// Ollama èŠå¤©è¯·æ±‚
#[derive(Debug, Serialize)]
struct OllamaChatRequest {
    model: String,
    messages: Vec<OllamaMessage>,
    stream: bool,
    #[serde(skip_serializing_if = "Option::is_none")]
    options: Option<OllamaOptions>,
}

/// Ollama æ¶ˆæ¯
#[derive(Debug, Serialize)]
struct OllamaMessage {
    role: String,
    content: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    images: Option<Vec<String>>,
}

/// Ollama é€‰é¡¹
#[derive(Debug, Serialize)]
struct OllamaOptions {
    temperature: f32,
    num_predict: i32, // Ollama ä½¿ç”¨ num_predict è€Œä¸æ˜¯ max_tokens
    #[serde(skip_serializing_if = "Option::is_none")]
    stop: Option<Vec<String>>, // åœæ­¢è¯
}

/// Ollama èŠå¤©å“åº”
#[derive(Debug, Deserialize)]
struct OllamaChatResponse {
    message: OllamaResponseMessage,
    #[serde(default)]
    done: bool,
}

/// Ollama å“åº”æ¶ˆæ¯
#[derive(Debug, Deserialize)]
struct OllamaResponseMessage {
    role: String,
    content: String,
    #[serde(default)]
    thinking: Option<String>, // qwen3-vl è¿”å›çš„æ€è€ƒè¿‡ç¨‹
}

/// Ollama å®¢æˆ·ç«¯ (åŸç”Ÿ API)
pub struct OllamaClient {
    base_url: String,
    settings: ModelConfig,
    client: reqwest::Client,
}

impl OllamaClient {
    /// åˆ›å»ºæ–°çš„ Ollama å®¢æˆ·ç«¯
    pub fn new(settings: ModelConfig) -> Result<Self> {
        // ç§»é™¤ /v1 åç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå› ä¸º Ollama åŸç”Ÿ API ä¸ä½¿ç”¨ /v1
        let mut base_url = settings.api_base.trim_end_matches('/').to_string();
        if base_url.ends_with("/v1") {
            base_url = base_url.trim_end_matches("/v1").to_string();
            log::info!("ğŸ¦™ æ£€æµ‹åˆ° /v1 åç¼€ï¼Œå·²è‡ªåŠ¨ç§»é™¤ï¼ˆOllama åŸç”Ÿ API ä¸éœ€è¦ï¼‰");
        }

        log::info!("ğŸ¦™ åˆ›å»º Ollama å®¢æˆ·ç«¯");
        log::info!("   Base URL: {}", base_url);
        log::info!("   æ¨¡å‹: {}", settings.model_name);

        Ok(Self {
            base_url,
            settings,
            client: reqwest::Client::new(),
        })
    }

    /// è°ƒç”¨ Ollama æ¨¡å‹ (çº¯æ–‡æœ¬)
    pub async fn chat(&self, system_prompt: &str, user_prompt: &str) -> Result<String> {
        log::info!("ğŸ¦™ è°ƒç”¨ Ollama API: {}", self.settings.model_name);

        let messages = vec![
            OllamaMessage {
                role: "system".to_string(),
                content: system_prompt.to_string(),
                images: None,
            },
            OllamaMessage {
                role: "user".to_string(),
                content: user_prompt.to_string(),
                images: None,
            },
        ];

        let request = OllamaChatRequest {
            model: self.settings.model_name.clone(),
            messages,
            stream: false,
            options: Some(OllamaOptions {
                temperature: self.settings.temperature,
                num_predict: self.settings.max_tokens as i32,
                stop: None,
            }),
        };

        let url = format!("{}/api/chat", self.base_url);
        log::debug!("ğŸ“¤ è¯·æ±‚ URL: {}", url);

        let response = self
            .client
            .post(&url)
            .json(&request)
            .send()
            .await
            .map_err(|e| anyhow!("Ollama è¯·æ±‚å¤±è´¥: {}", e))?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            return Err(anyhow!("Ollama API è¿”å›é”™è¯¯ {}: {}", status, error_text));
        }

        let ollama_response: OllamaChatResponse = response
            .json()
            .await
            .map_err(|e| anyhow!("è§£æ Ollama å“åº”å¤±è´¥: {}", e))?;

        log::info!("âœ… Ollama å“åº”æˆåŠŸ");

        // æå–å†…å®¹: content æ˜¯çœŸæ­£çš„ç­”æ¡ˆ
        let content = ollama_response.message.content;

        // è®°å½• thinking ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if let Some(ref thinking) = ollama_response.message.thinking {
            log::debug!("ğŸ§  æ¨¡å‹è¿”å›äº† thinking å­—æ®µ: {} bytes", thinking.len());
        }

        if content.is_empty() {
            log::warn!("âš ï¸  Ollama è¿”å›äº†ç©º content");
            return Err(anyhow!("AI å“åº”ä¸ºç©ºï¼Œè¯·é‡è¯•"));
        }

        log::info!("ğŸ“ å“åº”é•¿åº¦: {} bytes", content.len());

        Ok(content)
    }

    /// è°ƒç”¨ Ollama Vision æ¨¡å‹ (å¸¦å›¾ç‰‡)
    pub async fn chat_with_vision(
        &self,
        system_prompt: &str,
        user_prompt: &str,
        image_base64: &str,
    ) -> Result<String> {
        log::info!("ğŸ‘ï¸  è°ƒç”¨ Ollama Vision API: {}", self.settings.model_name);

        // å»æ‰ data URL å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        let clean_base64 = if image_base64.contains("base64,") {
            image_base64.split("base64,").nth(1).unwrap_or(image_base64)
        } else {
            image_base64
        };

        log::debug!(
            "ğŸ“¤ Base64 æ•°æ®é•¿åº¦: {} bytes (åŸå§‹: {})",
            clean_base64.len(),
            image_base64.len()
        );

        let messages = vec![
            OllamaMessage {
                role: "system".to_string(),
                content: system_prompt.to_string(),
                images: None,
            },
            OllamaMessage {
                role: "user".to_string(),
                content: user_prompt.to_string(),
                images: Some(vec![clean_base64.to_string()]),
            },
        ];

        let request = OllamaChatRequest {
            model: self.settings.model_name.clone(),
            messages,
            stream: false,
            options: Some(OllamaOptions {
                temperature: self.settings.temperature,
                num_predict: self.settings.max_tokens as i32,
                stop: None,
            }),
        };

        let url = format!("{}/api/chat", self.base_url);
        log::debug!("ğŸ“¤ è¯·æ±‚ URL: {}", url);

        let response = self
            .client
            .post(&url)
            .json(&request)
            .timeout(std::time::Duration::from_secs(120))
            .send()
            .await
            .map_err(|e| anyhow!("Ollama Vision è¯·æ±‚å¤±è´¥: {}", e))?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            log::error!("âŒ Ollama Vision API é”™è¯¯ {}: {}", status, error_text);
            return Err(anyhow!(
                "Ollama Vision API è¿”å›é”™è¯¯ {}: {}",
                status,
                error_text
            ));
        }

        // å…ˆè·å–åŸå§‹æ–‡æœ¬ï¼Œç”¨äºè°ƒè¯•
        let response_text = response
            .text()
            .await
            .map_err(|e| anyhow!("è¯»å– Ollama Vision å“åº”å¤±è´¥: {}", e))?;

        log::info!("ğŸ“¥ åŸå§‹å“åº”é•¿åº¦: {} bytes", response_text.len());
        // å®‰å…¨æˆªå–å‰ 300 ä¸ªå­—ç¬¦ (å¤„ç†å¤šå­—èŠ‚å­—ç¬¦)
        let preview_len = response_text.len().min(300);
        let mut safe_preview_len = preview_len;
        while safe_preview_len > 0 && !response_text.is_char_boundary(safe_preview_len) {
            safe_preview_len -= 1;
        }
        log::info!("ğŸ“¥ åŸå§‹å“åº”é¢„è§ˆ: {}", &response_text[..safe_preview_len]);

        // è§£æ JSON
        let ollama_response: OllamaChatResponse =
            serde_json::from_str(&response_text).map_err(|e| {
                anyhow!(
                    "è§£æ Ollama Vision å“åº”å¤±è´¥: {} | å“åº”: {}",
                    e,
                    &response_text[..response_text.len().min(200)]
                )
            })?;

        log::info!("âœ… Ollama Vision å“åº”æˆåŠŸ");

        // æå–å†…å®¹: content æ˜¯çœŸæ­£çš„ç­”æ¡ˆ
        let content = ollama_response.message.content;

        // è®°å½• thinking ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ç”¨äºè°ƒè¯•
        if let Some(ref thinking) = ollama_response.message.thinking {
            log::debug!("ğŸ§  æ¨¡å‹è¿”å›äº† thinking å­—æ®µ: {} bytes", thinking.len());
            // åªé¢„è§ˆå‰ 100 ä¸ªå­—ç¬¦
            let preview_len = thinking.len().min(100);
            let mut safe_len = preview_len;
            while safe_len > 0 && !thinking.is_char_boundary(safe_len) {
                safe_len -= 1;
            }
            log::debug!("ğŸ§  thinking é¢„è§ˆ: {}...", &thinking[..safe_len]);
        }

        log::info!("ğŸ“¥ æå–çš„ content é•¿åº¦: {} bytes", content.len());
        if content.len() > 0 {
            // å®‰å…¨æˆªå–å‰ 200 ä¸ªå­—ç¬¦
            let preview_len = content.len().min(200);
            let mut safe_len = preview_len;
            while safe_len > 0 && !content.is_char_boundary(safe_len) {
                safe_len -= 1;
            }
            log::info!("ğŸ“¥ content å‰{}å­—ç¬¦: {}", safe_len, &content[..safe_len]);
        }

        if content.is_empty() {
            log::error!("âš ï¸  Ollama Vision è¿”å›äº†ç©º content!");
            return Err(anyhow!("AI è§†è§‰å“åº”ä¸ºç©ºï¼Œè¯·é‡è¯•"));
        }

        Ok(content)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use base64::Engine;
    use std::fs;
    use std::path::Path; // å¯¼å…¥ base64 Engine trait

    #[tokio::test]
    #[ignore] // éœ€è¦æœ¬åœ° Ollama æœåŠ¡è¿è¡Œ
    async fn test_ollama_chat() {
        let settings = ModelConfig {
            provider: "local".to_string(),
            api_base: "http://localhost:11434".to_string(),
            api_key: None,
            model_name: "qwen3-vl:latest".to_string(),
            enabled: true,
            temperature: 0.7,
            max_tokens: 100,
        };

        let client = OllamaClient::new(settings).unwrap();
        let response = client.chat("ä½ æ˜¯æ¸¸æˆåŠ©æ‰‹", "ä½ å¥½").await;

        assert!(response.is_ok());
        println!("å“åº”: {}", response.unwrap());
    }

    #[tokio::test]
    async fn test_ollama_vision_with_real_image() {
        // åˆå§‹åŒ–æ—¥å¿—
        let _ = env_logger::builder()
            .filter_level(log::LevelFilter::Debug)
            .is_test(true)
            .try_init();

        println!("\nğŸ§ª å¼€å§‹æµ‹è¯• Ollama Vision API");
        println!("{}", "=".repeat(60));

        // 1. è¯»å–å›¾ç‰‡æ–‡ä»¶
        let image_path = Path::new(r"C:\Users\Administrator\Downloads\1.png");
        println!("\nğŸ“ è¯»å–å›¾ç‰‡: {}", image_path.display());

        let image_data = fs::read(image_path).expect("æ— æ³•è¯»å–å›¾ç‰‡æ–‡ä»¶,è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®");
        println!("âœ… å›¾ç‰‡è¯»å–æˆåŠŸ: {} bytes", image_data.len());

        // 2. è½¬æ¢ä¸º Base64
        let base64_image = base64::engine::general_purpose::STANDARD.encode(&image_data);
        println!("âœ… Base64 ç¼–ç æˆåŠŸ: {} chars", base64_image.len());

        // 3. åˆ›å»º Ollama å®¢æˆ·ç«¯
        let settings = ModelConfig {
            provider: "local".to_string(),
            api_base: "http://localhost:11434".to_string(),
            api_key: None,
            model_name: "qwen3-vl:latest".to_string(),
            enabled: true,
            temperature: 0.7,
            max_tokens: 2000,
        };

        let client = OllamaClient::new(settings).unwrap();
        println!("\nâœ… Ollama å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ");

        // 4. è°ƒç”¨ Vision API
        println!("\nğŸ”® è°ƒç”¨ Vision API...");
        println!("   System: ä½ æ˜¯æ¸¸æˆåŠ©æ‰‹");
        println!("   User: è¿™æ˜¯ä»€ä¹ˆæ¸¸æˆ?è¯·è¯¦ç»†æè¿°");

        let result = client
            .chat_with_vision("ä½ æ˜¯æ¸¸æˆåŠ©æ‰‹", "è¿™æ˜¯ä»€ä¹ˆæ¸¸æˆ?è¯·è¯¦ç»†æè¿°", &base64_image)
            .await;

        // 5. æ£€æŸ¥ç»“æœ
        println!("\nğŸ“Š æµ‹è¯•ç»“æœ:");
        println!("{}", "=".repeat(60));

        match result {
            Ok(response) => {
                println!("âœ… æˆåŠŸ!");
                println!("\nğŸ“ AI å›å¤:");
                println!("{}", "-".repeat(60));
                println!("{}", response);
                println!("{}", "-".repeat(60));
                println!("\nğŸ“ å›å¤é•¿åº¦: {} å­—ç¬¦", response.len());

                // éªŒè¯å›å¤ä¸ä¸ºç©º
                assert!(!response.is_empty(), "å“åº”å†…å®¹ä¸åº”è¯¥ä¸ºç©º");
                assert!(response.len() > 10, "å“åº”å†…å®¹å¤ªçŸ­,å¯èƒ½æœ‰é—®é¢˜");

                println!("\nâœ… AI æˆåŠŸè¯†åˆ«äº†å›¾ç‰‡å†…å®¹!");
            }
            Err(e) => {
                println!("âŒ å¤±è´¥!");
                println!("é”™è¯¯: {}", e);
                panic!("Vision API è°ƒç”¨å¤±è´¥: {}", e);
            }
        }

        println!("\nâœ… æµ‹è¯•å®Œæˆ!");
    }
}
