use anyhow::{Result, anyhow};
use async_openai::{
    config::OpenAIConfig,
    types::{
        ChatCompletionRequestMessage, ChatCompletionRequestSystemMessageArgs,
        ChatCompletionRequestUserMessageArgs, CreateChatCompletionRequestArgs,
        ChatCompletionRequestUserMessageContent, ImageDetail,
    },
    Client,
};
use crate::settings::ModelConfig;

/// OpenAI å®¢æˆ·ç«¯
pub struct OpenAIClient {
    client: Client<OpenAIConfig>,
    settings: ModelConfig,
}

impl OpenAIClient {
    /// åˆ›å»ºæ–°çš„ OpenAI å®¢æˆ·ç«¯
    pub fn new(settings: ModelConfig) -> Result<Self> {
        // åˆ›å»ºé…ç½®
        let mut config = OpenAIConfig::new()
            .with_api_base(&settings.api_base);

        // åªæœ‰åœ¨æä¾›äº† API Key æ—¶æ‰è®¾ç½® (æœ¬åœ° Ollama ä¸éœ€è¦)
        if let Some(api_key) = &settings.api_key {
            config = config.with_api_key(api_key);
        } else {
            // æœ¬åœ°æ¨¡å‹ä½¿ç”¨å ä½ç¬¦ API Key (Ollama ä¼šå¿½ç•¥)
            config = config.with_api_key("ollama");
        }

        let client = Client::with_config(config);

        Ok(Self { client, settings })
    }

    /// è°ƒç”¨ GPT æ¨¡å‹ (çº¯æ–‡æœ¬)
    pub async fn chat(&self, system_prompt: &str, user_prompt: &str) -> Result<String> {
        log::info!("ğŸ¤– è°ƒç”¨ OpenAI API: {}", self.settings.model_name);

        let messages = vec![
            ChatCompletionRequestMessage::System(
                ChatCompletionRequestSystemMessageArgs::default()
                    .content(system_prompt)
                    .build()?
            ),
            ChatCompletionRequestMessage::User(
                ChatCompletionRequestUserMessageArgs::default()
                    .content(user_prompt)
                    .build()?
            ),
        ];

        let request = CreateChatCompletionRequestArgs::default()
            .model(&self.settings.model_name)
            .messages(messages)
            .temperature(self.settings.temperature)
            .max_tokens(self.settings.max_tokens)
            .build()?;

        let response = self.client
            .chat()
            .create(request)
            .await
            .map_err(|e| anyhow!("OpenAI API è°ƒç”¨å¤±è´¥: {}", e))?;

        let content = response
            .choices
            .first()
            .and_then(|choice| choice.message.content.clone())
            .ok_or_else(|| anyhow!("OpenAI è¿”å›ç©ºå†…å®¹"))?;

        log::info!("âœ… OpenAI å“åº”æˆåŠŸ ({} tokens)", 
            response.usage.map(|u| u.total_tokens).unwrap_or(0));

        Ok(content)
    }

    /// è°ƒç”¨ GPT Vision æ¨¡å‹ (å¸¦å›¾ç‰‡)
    pub async fn chat_with_vision(
        &self,
        system_prompt: &str,
        user_prompt: &str,
        image_base64: &str,
    ) -> Result<String> {
        log::info!("ğŸ‘ï¸  è°ƒç”¨ OpenAI Vision API: {}", self.settings.model_name);

        // æ„å»ºå›¾ç‰‡ URL (data URL æ ¼å¼)
        let image_url = format!("data:image/jpeg;base64,{}", image_base64);

        let messages = vec![
            ChatCompletionRequestMessage::System(
                ChatCompletionRequestSystemMessageArgs::default()
                    .content(system_prompt)
                    .build()?
            ),
            ChatCompletionRequestMessage::User(
                ChatCompletionRequestUserMessageArgs::default()
                    .content(ChatCompletionRequestUserMessageContent::Array(vec![
                        // æ–‡æœ¬å†…å®¹
                        async_openai::types::ChatCompletionRequestMessageContentPart::Text(
                            async_openai::types::ChatCompletionRequestMessageContentPartText {
                                text: user_prompt.to_string(),
                            }
                        ),
                        // å›¾ç‰‡å†…å®¹
                        async_openai::types::ChatCompletionRequestMessageContentPart::ImageUrl(
                            async_openai::types::ChatCompletionRequestMessageContentPartImage {
                                image_url: async_openai::types::ImageUrl {
                                    url: image_url,
                                    detail: Some(ImageDetail::Auto),
                                }
                            }
                        ),
                    ]))
                    .build()?
            ),
        ];

        let request = CreateChatCompletionRequestArgs::default()
            .model(&self.settings.model_name)
            .messages(messages)
            .temperature(self.settings.temperature)
            .max_tokens(self.settings.max_tokens)
            .build()?;

        let response = self.client
            .chat()
            .create(request)
            .await
            .map_err(|e| anyhow!("OpenAI Vision API è°ƒç”¨å¤±è´¥: {}", e))?;

        let content = response
            .choices
            .first()
            .and_then(|choice| choice.message.content.clone())
            .ok_or_else(|| anyhow!("OpenAI Vision è¿”å›ç©ºå†…å®¹"))?;

        log::info!("âœ… OpenAI Vision å“åº”æˆåŠŸ ({} tokens)", 
            response.usage.map(|u| u.total_tokens).unwrap_or(0));

        Ok(content)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    #[ignore] // éœ€è¦çœŸå® API Key æ‰èƒ½è¿è¡Œ
    async fn test_openai_chat() {
        let settings = ModelConfig {
            provider: "openai".to_string(),
            api_base: "https://api.openai.com/v1".to_string(),
            api_key: Some("sk-...".to_string()), // æ›¿æ¢ä¸ºçœŸå® API Key
            model_name: "gpt-4o-mini".to_string(),
            enabled: true,
            temperature: 0.7,
            max_tokens: 500,
        };

        let client = OpenAIClient::new(settings).unwrap();
        let response = client.chat(
            "ä½ æ˜¯ä¸€ä¸ªæ¸¸æˆåŠ©æ‰‹ã€‚",
            "ç®€å•ä»‹ç»ä¸€ä¸‹æé¬¼ç—‡æ¸¸æˆã€‚"
        ).await;

        assert!(response.is_ok());
        println!("å“åº”: {}", response.unwrap());
    }
}
