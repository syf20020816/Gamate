use crate::crawler::types::*;
use crate::crawler::utils::*;
use reqwest::Client;
use serde::Deserialize;
use std::time::{SystemTime, UNIX_EPOCH};

/// Fandom MediaWiki API ÂìçÂ∫î
#[derive(Debug, Deserialize)]
struct ApiResponse {
    query: Option<QueryResponse>,
}

#[derive(Debug, Deserialize)]
struct QueryResponse {
    pages: std::collections::HashMap<String, PageData>,
}

#[derive(Debug, Deserialize)]
struct PageData {
    pageid: u64,
    title: String,
    revisions: Option<Vec<RevisionData>>,
    categories: Option<Vec<CategoryData>>,
}

#[derive(Debug, Deserialize)]
struct RevisionData {
    #[serde(rename = "*")]
    content: Option<String>,
    slots: Option<Slots>,
}

#[derive(Debug, Deserialize)]
struct Slots {
    main: Option<MainSlot>,
}

#[derive(Debug, Deserialize)]
struct MainSlot {
    #[serde(rename = "*")]
    content: String,
}

#[derive(Debug, Deserialize)]
struct CategoryData {
    title: String,
}

/// Fandom API Áà¨Ëô´
pub struct FandomApiCrawler {
    config: CrawlerConfig,
    client: Client,
    entries: Vec<WikiEntry>,
}

impl FandomApiCrawler {
    pub fn new(config: CrawlerConfig) -> Self {
        let client = Client::builder()
            .user_agent("GamePartnerSkill/1.0 (https://github.com/your-repo)")
            .timeout(std::time::Duration::from_secs(30))
            .build()
            .unwrap();

        Self {
            config,
            client,
            entries: Vec::new(),
        }
    }

    /// ÂºÄÂßãÁà¨Âèñ
    pub async fn crawl(&mut self) -> CrawlerResult2<CrawlerResult> {
        let start = std::time::Instant::now();
        let mut details = Vec::new();

        log::info!("üöÄ ÂºÄÂßã‰ΩøÁî® Fandom API Áà¨Âèñ: {}", self.config.source_url);

        // ‰ªé URL ÊèêÂèñ wiki Âü∫Á°ÄÂú∞ÂùÄ
        // ‰æãÂ¶Ç: https://phasmophobia.fandom.com/wiki/ -> https://phasmophobia.fandom.com/api.php
        let api_url = self.config.source_url
            .replace("/wiki/", "/api.php");
        
        log::info!("üì° API URL: {}", api_url);
        log::info!("‚öôÔ∏è  ÊúÄÂ§ßÈ°µÈù¢Êï∞: {}", self.config.max_pages);

        // 1. Ëé∑ÂèñÊâÄÊúâÈ°µÈù¢ÂàóË°®
        log::info!("üìã Ê≠£Âú®Ëé∑ÂèñÈ°µÈù¢ÂàóË°®...");
        let page_titles = self.fetch_all_pages(&api_url).await?;
        log::info!("‚úÖ ÊâæÂà∞ {} ‰∏™È°µÈù¢", page_titles.len());
        
        if page_titles.is_empty() {
            log::error!("‚ùå Êú™ÊâæÂà∞‰ªª‰ΩïÈ°µÈù¢ÔºÅ");
            log::error!("   ËØ∑Ê£ÄÊü•:");
            log::error!("   1. source_url: {}", self.config.source_url);
            log::error!("   2. api_url: {}", api_url);
            log::error!("   3. ÁΩëÁªúËøûÊé•ÊòØÂê¶Ê≠£Â∏∏");
            return Ok(CrawlerResult {
                total_entries: 0,
                total_bytes: 0,
                duration_secs: start.elapsed().as_secs(),
                error_count: 0,
                storage_path: self.config.storage_path.to_string_lossy().to_string(),
                details: vec!["ÈîôËØØ: Êú™ÊâæÂà∞‰ªª‰ΩïÈ°µÈù¢".to_string()],
            });
        }
        
        details.push(format!("ÊÄªÈ°µÈù¢Êï∞: {}", page_titles.len()));

        // 2. ÊâπÈáèËé∑ÂèñÈ°µÈù¢ÂÜÖÂÆπ
        let max_pages = self.config.max_pages.min(page_titles.len());
        log::info!("üìÑ Ê≠£Âú®Ëé∑Âèñ {} ‰∏™È°µÈù¢ÁöÑÂÜÖÂÆπÔºàÂÖ±{}‰∏™Ôºâ...", max_pages, page_titles.len());
        
        for (i, chunk) in page_titles[..max_pages].chunks(50).enumerate() {
            log::info!("   ÊâπÊ¨° {}: Ëé∑Âèñ {} ‰∏™È°µÈù¢...", i + 1, chunk.len());
            self.fetch_pages_content(&api_url, chunk).await?;
            log::info!("   ÊâπÊ¨° {} ÂÆåÊàêÔºåÂΩìÂâçÂÖ± {} ‰∏™Êù°ÁõÆ", i + 1, self.entries.len());
            
            // Âª∂ËøüÈÅøÂÖçÈôêÊµÅ
            tokio::time::sleep(std::time::Duration::from_millis(self.config.request_delay_ms)).await;
        }

        // ‰øùÂ≠òÁªìÊûú
        log::info!("üíæ Ê≠£Âú®‰øùÂ≠ò {} ‰∏™Êù°ÁõÆÂà∞Êñá‰ª∂...", self.entries.len());
        let total_bytes = self.save_entries()?;
        let duration_secs = start.elapsed().as_secs();

        log::info!(
            "üéâ Áà¨ÂèñÂÆåÊàê: {} Êù°ÁõÆ, {} Â≠óËäÇ, {} Áßí",
            self.entries.len(),
            total_bytes,
            duration_secs
        );

        details.push(format!("ÊàêÂäüÊù°ÁõÆÊï∞: {}", self.entries.len()));
        details.push(format!("ÊÄªÂ≠óËäÇÊï∞: {}", total_bytes));
        details.push(format!("ËÄóÊó∂: {} Áßí", duration_secs));

        Ok(CrawlerResult {
            total_entries: self.entries.len(),
            total_bytes,
            duration_secs,
            error_count: 0,
            storage_path: self.config.storage_path.to_string_lossy().to_string(),
            details,
        })
    }

    /// Ëé∑ÂèñÊâÄÊúâÈ°µÈù¢Ê†áÈ¢ò
    async fn fetch_all_pages(&self, api_url: &str) -> CrawlerResult2<Vec<String>> {
        let mut all_titles = Vec::new();
        let mut continue_token: Option<String> = None;

        log::info!("Ê≠£Âú®‰ªé {} Ëé∑ÂèñÈ°µÈù¢ÂàóË°®...", api_url);

        loop {
            let mut params = vec![
                ("action", "query"),
                ("format", "json"),
                ("list", "allpages"),
                ("aplimit", "500"), // ÊØèÊ¨°Ëé∑Âèñ500‰∏™
                ("apnamespace", "0"), // Âè™Ë¶Å‰∏ªÂëΩÂêçÁ©∫Èó¥ÔºàÊñáÁ´†Ôºâ
            ];

            if let Some(ref token) = continue_token {
                params.push(("apcontinue", token));
            }

            let response = self.client
                .get(api_url)
                .query(&params)
                .send()
                .await?;

            let status = response.status();
            log::info!("API ÂìçÂ∫îÁä∂ÊÄÅ: {}", status);

            if !status.is_success() {
                log::error!("API ËøîÂõûÈîôËØØÁä∂ÊÄÅ: {}", status);
                return Err(CrawlerError::HttpError(
                    reqwest::Error::from(response.error_for_status().unwrap_err()),
                ));
            }

            let json: serde_json::Value = response.json().await?;
            
            // Ë∞ÉËØïÔºöÊâìÂç∞ÂìçÂ∫îÁªìÊûÑ
            log::debug!("API ÂìçÂ∫î: {}", serde_json::to_string_pretty(&json).unwrap_or_default());

            // ÊèêÂèñÈ°µÈù¢Ê†áÈ¢ò
            if let Some(pages) = json["query"]["allpages"].as_array() {
                log::info!("Êú¨Ê¨°Ëé∑Âèñ {} ‰∏™È°µÈù¢", pages.len());
                for page in pages {
                    if let Some(title) = page["title"].as_str() {
                        all_titles.push(title.to_string());
                    }
                }
            } else {
                log::warn!("Êú™ÊâæÂà∞ query.allpages Â≠óÊÆµ");
                log::debug!("ÂìçÂ∫îÁªìÊûÑ: {:?}", json);
            }

            // Ê£ÄÊü•ÊòØÂê¶ÊúâÊõ¥Â§öÈ°µÈù¢
            if let Some(continue_obj) = json["continue"].as_object() {
                if let Some(token) = continue_obj["apcontinue"].as_str() {
                    continue_token = Some(token.to_string());
                    log::info!("Â∑≤Ëé∑Âèñ {} ‰∏™È°µÈù¢Ê†áÈ¢òÔºåÁªßÁª≠...", all_titles.len());
                } else {
                    break;
                }
            } else {
                break;
            }
        }

        log::info!("ÊÄªÂÖ±Ëé∑Âèñ {} ‰∏™È°µÈù¢Ê†áÈ¢ò", all_titles.len());
        Ok(all_titles)
    }

    /// ÊâπÈáèËé∑ÂèñÈ°µÈù¢ÂÜÖÂÆπ
    async fn fetch_pages_content(&mut self, api_url: &str, titles: &[String]) -> CrawlerResult2<()> {
        let titles_str = titles.join("|");
        
        log::info!("Ëé∑Âèñ {} ‰∏™È°µÈù¢ÁöÑÂÜÖÂÆπ...", titles.len());

        // ‰øÆÊîπÔºö‰ΩøÁî® revisions ËÄå‰∏çÊòØ extracts
        let params = vec![
            ("action", "query"),
            ("format", "json"),
            ("prop", "revisions|categories"),
            ("titles", &titles_str),
            ("rvprop", "content"), // Ëé∑Âèñ‰øÆËÆ¢ÂÜÖÂÆπ
            ("rvslots", "main"), // Ëé∑Âèñ‰∏ªÊßΩ‰Ωç
            ("cllimit", "50"), // ÊúÄÂ§ö50‰∏™ÂàÜÁ±ª
            ("redirects", "1"), // Ëá™Âä®Ë∑üÈöèÈáçÂÆöÂêë
        ];

        let response = self.client
            .get(api_url)
            .query(&params)
            .send()
            .await?;

        let status = response.status();
        log::info!("ÂÜÖÂÆπ API ÂìçÂ∫îÁä∂ÊÄÅ: {}", status);

        if !status.is_success() {
            log::error!("ÂÜÖÂÆπ API ËøîÂõûÈîôËØØ: {}", status);
            return Err(CrawlerError::HttpError(
                reqwest::Error::from(response.error_for_status().unwrap_err()),
            ));
        }

        // ÂÖàËé∑ÂèñÂéüÂßã JSON Êù•Ë∞ÉËØï
        let json: serde_json::Value = response.json().await?;
        log::debug!("üìù ÂéüÂßã API ÂìçÂ∫î: {}", serde_json::to_string_pretty(&json).unwrap_or_default());
        
        // Â∞ùËØïËß£Êûê
        let api_response: ApiResponse = serde_json::from_value(json.clone())
            .map_err(|e| {
                log::error!("‚ùå Ëß£Êûê API ÂìçÂ∫îÂ§±Ë¥•: {}", e);
                log::error!("ÂìçÂ∫îÂÜÖÂÆπ: {:?}", json);
                CrawlerError::Other(format!("Ëß£ÊûêÂ§±Ë¥•: {}", e))
            })?;

        if let Some(query) = api_response.query {
            log::info!("Êî∂Âà∞ {} ‰∏™È°µÈù¢ÁöÑÊï∞ÊçÆ", query.pages.len());
            
            let mut success_count = 0;
            let mut no_content_count = 0;
            
            for (page_id, page_data) in query.pages {
                log::debug!("Â§ÑÁêÜÈ°µÈù¢: {} (ID: {})", page_data.title, page_id);
                
                // ‰ªé revisions ‰∏≠ÊèêÂèñÂÜÖÂÆπ
                let content_opt = page_data.revisions
                    .and_then(|revisions| revisions.into_iter().next())
                    .and_then(|revision| {
                        // ‰ºòÂÖà‰ΩøÁî® slots.main.content
                        if let Some(slots) = revision.slots {
                            if let Some(main) = slots.main {
                                return Some(main.content);
                            }
                        }
                        // ÈôçÁ∫ßÔºö‰ΩøÁî®ÊóßÊ†ºÂºèÁöÑ content
                        revision.content
                    });
                
                if let Some(raw_content) = content_opt {
                    if raw_content.trim().is_empty() {
                        log::warn!("È°µÈù¢ {} ÁöÑÂÜÖÂÆπ‰∏∫Á©∫", page_data.title);
                        no_content_count += 1;
                        continue;
                    }
                    
                    let categories = page_data.categories
                        .unwrap_or_default()
                        .iter()
                        .map(|c| c.title.replace("Category:", ""))
                        .collect();

                    // Ê∏ÖÁêÜ Wiki Ê†áËÆ∞ËØ≠Ê≥ï
                    let content = clean_wiki_markup(&raw_content);
                    let hash = calculate_hash(&content);
                    let timestamp = SystemTime::now()
                        .duration_since(UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let entry = WikiEntry {
                        id: format!("{}_{}", self.config.game_id, hash),
                        title: page_data.title.clone(),
                        content,
                        url: format!("{}{}", self.config.source_url, page_data.title.replace(" ", "_")),
                        timestamp,
                        hash,
                        categories,
                        metadata: WikiMetadata {
                            length: raw_content.len(),
                            last_modified: None,
                            author: None,
                            language: "en".to_string(),
                        },
                    };

                    self.entries.push(entry);
                    success_count += 1;
                    log::debug!("‚úÖ ÊàêÂäüÊ∑ªÂä†Êù°ÁõÆ: {}", page_data.title);
                } else {
                    log::warn!("‚ö†Ô∏è  È°µÈù¢ {} Ê≤°ÊúâÂÜÖÂÆπÔºàÂèØËÉΩÊòØÈáçÂÆöÂêëÊàñÁâπÊÆäÈ°µÈù¢Ôºâ", page_data.title);
                    no_content_count += 1;
                }
            }
            
            log::info!("üìä Êú¨ÊâπÊ¨°: ÊàêÂäü {} ‰∏™ÔºåÊó†ÂÜÖÂÆπ {} ‰∏™", success_count, no_content_count);
        } else {
            log::warn!("API ÂìçÂ∫î‰∏≠Ê≤°Êúâ query Â≠óÊÆµ");
        }

        log::info!("ÂΩìÂâçÂ∑≤Ê∑ªÂä† {} ‰∏™Êù°ÁõÆ", self.entries.len());
        Ok(())
    }

    /// ‰øùÂ≠òÊù°ÁõÆÂà∞Êñá‰ª∂
    fn save_entries(&self) -> CrawlerResult2<usize> {
        std::fs::create_dir_all(&self.config.storage_path)?;

        let file_path = self.config.storage_path.join("wiki_raw.jsonl");
        let mut total_bytes = 0;

        let mut file_content = String::new();
        for entry in &self.entries {
            let json = serde_json::to_string(entry)
                .map_err(|e| CrawlerError::Other(e.to_string()))?;
            file_content.push_str(&json);
            file_content.push('\n');
            total_bytes += json.len() + 1;
        }

        std::fs::write(&file_path, file_content)?;

        // ‰øùÂ≠òÂÖÉÊï∞ÊçÆ
        let metadata = serde_json::json!({
            "game_id": self.config.game_id,
            "source_url": self.config.source_url,
            "source_type": "FandomAPI",
            "timestamp": self.config.timestamp,
            "total_entries": self.entries.len(),
            "total_bytes": total_bytes,
        });

        let metadata_path = self.config.storage_path.join("metadata.json");
        std::fs::write(
            &metadata_path,
            serde_json::to_string_pretty(&metadata)
                .map_err(|e| CrawlerError::Other(e.to_string()))?,
        )?;

        Ok(total_bytes)
    }
}
