use crate::rag::{build_rag_context, build_prompt, AIResponse, WikiReference};
use crate::settings::AppSettings;
use crate::llm::{OpenAIClient, OllamaClient};
use anyhow::Result;
use base64::{Engine as _, engine::general_purpose};

/// ç”Ÿæˆ AI å›å¤ (Tauri å‘½ä»¤)
#[tauri::command]
pub async fn generate_ai_response(
    message: String,
    game_id: String,
    screenshot: Option<String>,
) -> Result<AIResponse, String> {
    generate_ai_response_impl(message, game_id, screenshot)
        .await
        .map_err(|e| format!("AI å›å¤ç”Ÿæˆå¤±è´¥: {}", e))
}

/// ç”Ÿæˆ AI å›å¤ (å†…éƒ¨å®ç°)
async fn generate_ai_response_impl(
    message: String,
    game_id: String,
    screenshot: Option<String>,
) -> Result<AIResponse> {
    log::info!("ğŸ¤– å¼€å§‹ç”Ÿæˆ AI å›å¤");
    log::info!("   ç”¨æˆ·æ¶ˆæ¯: {}", message);
    log::info!("   æ¸¸æˆ ID: {}", game_id);

    // 1. æ„å»º RAG ä¸Šä¸‹æ–‡
    let context = build_rag_context(&game_id, &message, screenshot.clone()).await?;

    // 2. æ„å»º Prompt
    let game_name = get_game_name(&game_id);
    let (system_prompt, user_prompt) = build_prompt(&game_name, &message, &context);

    log::info!("ğŸ“ Prompt æ„å»ºå®Œæˆ");
    log::debug!("ç³»ç»Ÿ Prompt:\n{}", system_prompt);
    log::debug!("ç”¨æˆ· Prompt:\n{}", user_prompt);

    // 3. è°ƒç”¨ LLM
    let ai_content = call_llm(&system_prompt, &user_prompt, &screenshot).await?;

    // 4. è¿”å›ç»“æœ
    let wiki_references: Vec<WikiReference> = context
        .wiki_entries
        .into_iter()
        .map(|entry| WikiReference {
            title: entry.title,
            content: entry.content,
            score: entry.score,
            url: entry.url,
        })
        .collect();

    Ok(AIResponse {
        content: ai_content,
        wiki_references: Some(wiki_references),
    })
}

/// è·å–æ¸¸æˆåç§°
fn get_game_name(game_id: &str) -> String {
    match game_id {
        "phasmophobia" => "æé¬¼ç—‡",
        "elden-ring" => "è‰¾å°”ç™»æ³•ç¯",
        "dark-souls-3" => "é»‘æš—ä¹‹é­‚3",
        _ => "æœªçŸ¥æ¸¸æˆ",
    }
    .to_string()
}

/// å‡€åŒ– base64 å›¾ç‰‡å­—ç¬¦ä¸²
/// 
/// åŠŸèƒ½:
/// 1. å»é™¤ data:image/...;base64, å‰ç¼€ (å¦‚æœæœ‰)
/// 2. ç§»é™¤æ¢è¡Œç¬¦å’Œç©ºç™½å­—ç¬¦
/// 3. æ ¡éªŒ base64 æ ¼å¼æ˜¯å¦æœ‰æ•ˆ
/// 
/// è¿”å›: çº¯å‡€çš„ base64 å­—ç¬¦ä¸²
fn sanitize_base64_image(s: &str) -> Result<String> {
    let mut cleaned = s.trim().to_string();
    
    // 1. å»é™¤ data URL å‰ç¼€
    if let Some(comma_idx) = cleaned.find(',') {
        // å…ˆå¤åˆ¶å‰ç¼€ç”¨äºæ—¥å¿—,é¿å…å€Ÿç”¨å†²çª
        let prefix = cleaned[..comma_idx].to_string();
        if prefix.starts_with("data:") && prefix.contains("base64") {
            cleaned = cleaned[comma_idx + 1..].to_string();
            log::info!("ğŸ§¹ æ£€æµ‹åˆ° data URL å‰ç¼€,å·²ç§»é™¤: {}", prefix);
        }
    }
    
    // 2. ç§»é™¤æ‰€æœ‰æ¢è¡Œç¬¦å’Œç©ºç™½å­—ç¬¦
    cleaned.retain(|c| !c.is_whitespace());
    
    // 3. æ ¡éªŒ base64 æ ¼å¼
    match general_purpose::STANDARD.decode(&cleaned) {
        Ok(decoded) => {
            log::info!("âœ… base64 å›¾ç‰‡æ ¡éªŒæˆåŠŸ (è§£ç åå¤§å°: {} bytes)", decoded.len());
            Ok(cleaned)
        }
        Err(e) => {
            log::error!("âŒ base64 å›¾ç‰‡æ ¼å¼æ— æ•ˆ: {}", e);
            log::error!("   åŸå§‹å­—ç¬¦ä¸²é•¿åº¦: {}", s.len());
            log::error!("   æ¸…ç†åå­—ç¬¦ä¸²é•¿åº¦: {}", cleaned.len());
            log::error!("   å‰ 50 å­—ç¬¦: {}", &cleaned.chars().take(50).collect::<String>());
            Err(anyhow::anyhow!("æ— æ•ˆçš„ base64 å›¾ç‰‡æ ¼å¼: {}", e))
        }
    }
}

/// è°ƒç”¨ LLM (æ ¹æ®é…ç½®é€‰æ‹©ä¸åŒçš„å®ç°)
async fn call_llm(
    system_prompt: &str,
    user_prompt: &str,
    screenshot: &Option<String>,
) -> Result<String> {
    // åŠ è½½è®¾ç½®
    let settings = AppSettings::load()?;
    let multimodal_config = settings.ai_models.multimodal;

    // æ£€æŸ¥æ˜¯å¦å¯ç”¨
    if !multimodal_config.enabled {
        log::warn!("âš ï¸  å¤šæ¨¡æ€æ¨¡å‹æœªå¯ç”¨,ä½¿ç”¨ Mock å®ç°");
        return mock_llm_fallback(user_prompt);
    }

    // æ£€æŸ¥ API Key (ä»…å¯¹éæœ¬åœ°æ¨¡å‹)
    if multimodal_config.provider != "local" && multimodal_config.api_key.is_none() {
        log::warn!("âš ï¸  æœªé…ç½® API Key (æä¾›å•†: {}),ä½¿ç”¨ Mock å®ç°", multimodal_config.provider);
        return mock_llm_fallback(user_prompt);
    }

    // æ ¹æ® provider é€‰æ‹©åˆé€‚çš„å®¢æˆ·ç«¯
    let is_local = multimodal_config.provider == "local";
    
    log::info!("ğŸ¤– ä½¿ç”¨ {} å®¢æˆ·ç«¯", if is_local { "Ollama" } else { "OpenAI" });

    // å‡€åŒ– base64 å›¾ç‰‡ (å¦‚æœæœ‰æˆªå›¾)
    let clean_screenshot = if let Some(ref img) = screenshot {
        match sanitize_base64_image(img) {
            Ok(clean) => Some(clean),
            Err(e) => {
                log::error!("âŒ å›¾ç‰‡æ ¼å¼æ ¡éªŒå¤±è´¥: {}", e);
                return Err(anyhow::anyhow!("å›¾ç‰‡æ ¼å¼æ— æ•ˆ,è¯·é‡æ–°æˆªå›¾"));
            }
        }
    } else {
        None
    };

    // è°ƒç”¨ API (å¸¦é‡è¯•)
    for attempt in 1..=3 {
        log::info!("ğŸ”„ å°è¯•è°ƒç”¨ LLM API (ç¬¬ {}/3 æ¬¡)", attempt);

        let result = if is_local {
            // ä½¿ç”¨ Ollama åŸç”Ÿå®¢æˆ·ç«¯
            let client = match OllamaClient::new(multimodal_config.clone()) {
                Ok(c) => c,
                Err(e) => {
                    log::error!("âŒ åˆ›å»º Ollama å®¢æˆ·ç«¯å¤±è´¥: {}", e);
                    if attempt < 3 {
                        tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;
                        continue;
                    }
                    log::warn!("   å›é€€åˆ° Mock å®ç°");
                    return mock_llm_fallback(user_prompt);
                }
            };

            if let Some(ref img) = clean_screenshot {
                client.chat_with_vision(system_prompt, user_prompt, img).await
            } else {
                client.chat(system_prompt, user_prompt).await
            }
        } else {
            // ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯
            let client = match OpenAIClient::new(multimodal_config.clone()) {
                Ok(c) => c,
                Err(e) => {
                    log::error!("âŒ åˆ›å»º OpenAI å®¢æˆ·ç«¯å¤±è´¥: {}", e);
                    if attempt < 3 {
                        tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;
                        continue;
                    }
                    log::warn!("   å›é€€åˆ° Mock å®ç°");
                    return mock_llm_fallback(user_prompt);
                }
            };

            if let Some(ref img) = clean_screenshot {
                client.chat_with_vision(system_prompt, user_prompt, img).await
            } else {
                client.chat(system_prompt, user_prompt).await
            }
        };

        match result {
            Ok(content) => {
                log::info!("âœ… LLM API è°ƒç”¨æˆåŠŸ");
                return Ok(content);
            }
            Err(e) => {
                log::warn!("âš ï¸  ç¬¬ {} æ¬¡è°ƒç”¨å¤±è´¥: {}", attempt, e);
                if attempt < 3 {
                    // æŒ‡æ•°é€€é¿
                    let delay_ms = 1000 * (2_u64.pow(attempt - 1));
                    log::info!("   ç­‰å¾… {}ms åé‡è¯•...", delay_ms);
                    tokio::time::sleep(tokio::time::Duration::from_millis(delay_ms)).await;
                } else {
                    log::error!("âŒ LLM API è°ƒç”¨å¤±è´¥ (å·²é‡è¯• 3 æ¬¡): {}", e);
                    log::warn!("   å›é€€åˆ° Mock å®ç°");
                    return mock_llm_fallback(user_prompt);
                }
            }
        }
    }

    // ç†è®ºä¸Šä¸ä¼šåˆ°è¾¾è¿™é‡Œ
    mock_llm_fallback(user_prompt)
}

/// Mock LLM å›é€€å®ç°
fn mock_llm_fallback(user_prompt: &str) -> Result<String> {
    log::info!("âš ï¸  ä½¿ç”¨ Mock LLM å›é€€å®ç°");

    let mut response = String::new();
    
    // æ·»åŠ è¯­éŸ³æ’­æŠ¥ä¸“ç”¨æ ‡è®° (å‰ç«¯ä¼šè¯†åˆ«å¹¶ç®€åŒ–æ’­æŠ¥å†…å®¹)
    response.push_str("[TTS_SIMPLE]å¯¹è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API é…ç½®ã€‚[/TTS_SIMPLE]\n\n");
    
    // è¯¦ç»†ä¿¡æ¯ç”¨äºå±å¹•æ˜¾ç¤º
    response.push_str("## âš ï¸  AI API æœªé…ç½®æˆ–è°ƒç”¨å¤±è´¥\n\n");
    response.push_str("å½“å‰ä½¿ç”¨çš„æ˜¯ Mock AI å®ç°,æ— æ³•æä¾›æ™ºèƒ½å¯¹è¯ã€‚\n\n");
    response.push_str("**å¦‚ä½•å¯ç”¨çœŸå® AI:**\n\n");
    response.push_str("1. åœ¨è®¾ç½®é¡µé¢é…ç½® OpenAI API Key æˆ– Ollama æ¨¡å‹\n");
    response.push_str("2. é€‰æ‹©åˆé€‚çš„æ¨¡å‹ (æ¨è: gpt-4o-mini æˆ– llava)\n");
    response.push_str("3. ä¿å­˜è®¾ç½®åé‡æ–°å‘é€æ¶ˆæ¯\n\n");
    response.push_str("---\n\n");
    response.push_str(&format!("**æ‚¨çš„é—®é¢˜:** {}\n", user_prompt));

    Ok(response)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_mock_fallback() {
        let result = mock_llm_fallback("æµ‹è¯•é—®é¢˜");
        assert!(result.is_ok());
        let content = result.unwrap();
        assert!(content.contains("Mock AI"));
    }
}
